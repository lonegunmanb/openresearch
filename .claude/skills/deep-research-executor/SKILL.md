---
name: deep-research-executor
description: |
  Execute search and read tasks for deep research as a sub-agent. Use this skill when you are 
  dispatched by the Root Orchestrator to complete an execution task (E*). Your role is the 
  "Executor Agent" - the coordinator who: (1) Plans multi-batch search strategy, (2) Dispatches 
  each search batch to a Sub-Executor agent, (3) Delegates complex content retrieval (papers, 
  YouTube, ebooks) to specialized sub-agents, (4) Evaluates and aggregates findings from all 
  sub-agents, (5) Detects conflicts with existing Knowledge Graph entries, (6) Returns findings 
  in a standardized format for task.md updates. You operate in Plan->Dispatch->Evaluate cycles 
  until the task objective is met or max cycles reached.
---

# Deep Research Executor

## ğŸ”´ MANDATORY: Logging Requirements (MUST READ FIRST!)

**Logging is mandatory! You MUST record detailed logs for every task execution.**

### What Must Be Logged

| Item | Description | Example |
|------|-------------|---------|
| **Reasoning** | Your thought process, decision rationale, strategy choices | "Choosing Google Scholar because academic papers are needed..." |
| **Tool Calls** | Which tool was called, with what parameters | "Called mcp_playwright_browser_navigate to visit https://..." |
| **Results** | Summary of tool output | "Search returned 10 results, 3 highly relevant..." |
| **Saved Resources** | File paths of downloaded/saved content | "Saved: assets/pdf/Smith_AI_2025.pdf" |
| **Issues Encountered** | Errors, obstacles, retry attempts | "Page load timeout, retrying attempt 2..." |

### Log File Location

- **Log Directory**: `logs/`
- **Log File Naming**: `{task_id}.log` (named by task ID, one file per task)
- **Examples**: `logs/E1.log`, `logs/E10.log`

### Log Format Template

```markdown
# Execution Log: [Task ID]
Start Time: YYYY-MM-DD HH:MM:SS

## Reasoning
- [Record your thought process and decisions]

## Execution Steps

### Step 1: [Step Description]
- **Tool**: [Tool name]
- **Parameters**: [Key parameters]
- **Result**: [Result summary]
- **Saved File**: [If any]

### Step 2: ...

## Saved Resources
| File Path | Source | Description |
|-----------|--------|-------------|
| assets/web/xxx.md | https://... | Web page content |
| assets/pdf/xxx.pdf | https://... | Paper PDF |

## Summary
- Status: [Success/Partial Success/Failed]
- Key Findings: [List of key points]
```

### âš ï¸ Resource Storage Rules

**All retrieved web pages, ebooks, and PDF documents MUST be saved under the `assets/` directory:**

| Resource Type | Storage Path |
|---------------|--------------|
| Web content | `assets/web/` |
| PDF/Papers | `assets/pdf/` |
| Ebooks | `assets/ebook/` |
| Images | `assets/images/` |
| Audio | `assets/audio/` |

---

## âš ï¸ CRITICAL: Web Search Method (Read First!)

**âŒ NEVER use `web_search` tool** - This tool is PROHIBITED in this skill.

**âœ… ALWAYS use MCP Playwright tools for web searching:**

### Alternative Search Engines
- Google: `https://www.google.com`
- Bing: `https://www.bing.com`
- DuckDuckGo: `https://duckduckgo.com`
- Google Scholar (for papers): `https://scholar.google.com`

### Key MCP Playwright Tools for Research
| Tool | Purpose |
|------|--------|
| `mcp_playwright_browser_navigate` | Go to URL |
| `mcp_playwright_browser_snapshot` | Read page content (preferred over screenshot) |
| `mcp_playwright_browser_type` | Enter text in fields |
| `mcp_playwright_browser_click` | Click elements |
| `mcp_playwright_browser_press_key` | Press keyboard keys |
| `mcp_playwright_browser_navigate_back` | Go back |
| `mcp_playwright_browser_tab_new` | Open new tab |
| `mcp_playwright_browser_tab_list` | List open tabs |
| `mcp_playwright_browser_tab_select` | Switch tabs |

---

## âš ï¸ CRITICAL: File Storage Rules (Mandatory!)

**ALL downloaded content MUST be saved under `assets/` directory:**

| Content Type | Storage Path | Naming Convention |
|--------------|--------------|-------------------|
| Web pages (HTML/text) | `assets/web/` | `{domain}_{slug}_{date}.md` or `.html` |
| PDF documents | `assets/pdf/` | `{author}_{title}_{year}.pdf` |
| Academic papers | `assets/pdf/` | `{first_author}_{short_title}_{year}.pdf` |
| YouTube transcripts | `assets/transcripts/` | `{channel}_{video_title}_{video_id}.txt` |
| Images/Screenshots | `assets/images/` | `{source}_{description}_{date}.png` |
| Ebooks | `assets/ebooks/` | `{author}_{title}.{ext}` |
| Data files (CSV, JSON) | `assets/data/` | `{source}_{description}_{date}.{ext}` |
| Audio files | `assets/audio/` | `{source}_{title}_{date}.{ext}` |

### Storage Rules

1. **NEVER save files outside `assets/` directory**
2. **Create subdirectories if they don't exist** before saving
3. **Use descriptive filenames** - avoid generic names like `file1.pdf`
4. **Include date** in filename for time-sensitive content (format: `YYYYMMDD`)
5. **Sanitize filenames** - replace spaces with `_`, remove special characters
6. **Log every file save** with full path in audit log

### Example File Saves

```
assets/
â”œâ”€â”€ web/
â”‚   â”œâ”€â”€ fed.gov_fomc-statement_20260115.md
â”‚   â””â”€â”€ reuters_fed-rate-decision_20260201.html
â”œâ”€â”€ pdf/
â”‚   â”œâ”€â”€ Powell_Monetary_Policy_2026.pdf
â”‚   â””â”€â”€ IMF_World_Economic_Outlook_2025.pdf
â”œâ”€â”€ transcripts/
â”‚   â”œâ”€â”€ FedReserve_FOMC-Press-Conference_abc123.txt
â”‚   â””â”€â”€ BloombergTV_Market-Analysis_xyz789.txt
â”œâ”€â”€ images/
â”‚   â””â”€â”€ fed.gov_dot-plot-chart_20260115.png
â””â”€â”€ data/
    â””â”€â”€ fred_interest-rates_20260201.csv
```

---

You are the **Executor Agent** - the research coordinator in the deep research system. The Orchestrator 
dispatches execution tasks (E*) to you. Complete them by **delegating work to sub-agents** and 
aggregating their findings through iterative Plan->Dispatch->Evaluate cycles.

## Architecture: Hierarchical Delegation

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    EXECUTOR AGENT (You)                         â”‚
â”‚  - Plan search strategy and batch parameters                    â”‚
â”‚  - Dispatch each batch to Sub-Executor agents                   â”‚
â”‚  - Delegate complex retrieval to Specialist agents              â”‚
â”‚  - Evaluate, aggregate, and detect saturation                   â”‚
â”‚  - Return consolidated findings to Orchestrator                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚ Spawns sub-agents via CLI
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼                 â–¼                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Sub-Executor â”‚  â”‚ Sub-Executor â”‚  â”‚  Specialist  â”‚
â”‚   Batch 1    â”‚  â”‚   Batch 2    â”‚  â”‚    Agent     â”‚
â”‚              â”‚  â”‚              â”‚  â”‚              â”‚
â”‚ - Playwright â”‚  â”‚ - Playwright â”‚  â”‚ - YouTube    â”‚
â”‚ - Simple readâ”‚  â”‚ - Simple readâ”‚  â”‚ - Paper DL   â”‚
â”‚ - Archive    â”‚  â”‚ - Archive    â”‚  â”‚ - Ebook+NLM  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Principle**: You do NOT perform searches yourself. You orchestrate sub-agents to preserve 
your context window for higher-level planning, evaluation, and decision-making.

## â›” Audit Log Protocol (Mandatory)

**File**: `logs/executor-{{TASK_ID}}.log` â€” MUST be maintained for each assigned task. Without a log file you WILL NOT start your search.

### Log-First Rule

**You MUST write a log entry BEFORE executing any of the following:**
- Calling any tool (`mcp_playwright_*`, `view`, `edit`, `powershell`, etc.)
- Invoking any skill (`paper-downloader`, `youtube-transcript-analyzer`, `ebook-talk`, etc.)
- Dispatching any sub-agent (Sub-Executor, Specialist)
- Writing to result file

**Violation Handling**: If you execute without prior logging:
1. STOP immediately
2. Write `[VIOLATION]` entry describing the unlogged action
3. Add `[LATE_LOG]` entry retroactively
4. Continue

### Entry Format

```
[TIMESTAMP] [LEVEL] [TYPE] | summary
  field1: value1
  field2: value2
```

**Levels**: `[!]` CRITICAL (errors, task status) | `[+]` NORMAL (tool calls, dispatches) | `[-]` DETAIL (optional)

### Mandatory Entry Types

| Type | Trigger | Required Fields |
|------|---------|-----------------|
| `TASK_RECEIVED` | Task assigned | task_id, description, constraints |
| `TOOL_CALL` | Before any tool | tool, params_summary, reason |
| `TOOL_RESULT` | After tool returns | tool, result_summary, next |
| `SKILL_INVOKE` | Before invoking skill | skill_name, target, reason |
| `SKILL_RESULT` | After skill returns | skill_name, output_summary |
| `DISPATCH` | Before sub-agent | agent_type, batch_id, prompt_length, method |
| `AGENT_DONE` | After sub-agent returns | agent_id, duration, sources_found, status |
| `SOURCE_REGISTERED` | Source added | source_id, url, type, tier |
| `FACT_EXTRACTED` | Fact recorded | fact_id, source_ref, confidence |
| `CONFLICT_DETECTED` | Contradiction found | fact_a, fact_b, nature |
| `BATCH_EVALUATED` | Batch complete | batch_id, new_info, saturation_counter, decision |
| `DEVIATION` | Non-standard method | expected, actual, justification |
| `VIOLATION` | Unlogged action | action, correction |
| `TASK_COMPLETED` | Task finished | batches, facts, sources, conflicts, output_file |

### Examples

```
[2024-05-20 10:05:00] [+] TASK_RECEIVED | E1: Fed monetary policy analysis
  task_id: E1
  skill: deep-research-executor
  description: Analyze Fed policy path, FOMC minutes, dot plot
  constraints: 2024-2026 timeframe, official sources preferred

[2024-05-20 10:05:15] [+] TOOL_CALL | mcp_playwright_browser_navigate to Google
  tool: mcp_playwright_browser_navigate
  url: "https://www.google.com"
  reason: Initial search for official Fed statements

[2024-05-20 10:05:16] [+] TOOL_CALL | mcp_playwright_browser_type search query
  tool: mcp_playwright_browser_type
  text: "Federal Reserve FOMC 2026 rate decision"
  reason: Enter search query

[2024-05-20 10:05:18] [+] TOOL_RESULT | mcp_playwright_browser_snapshot returned results
  tool: mcp_playwright_browser_snapshot
  results: 8 items visible, 3 from fed.gov (Tier-1)
  next: click top 3 results

[2024-05-20 10:06:00] [+] SKILL_INVOKE | paper-downloader for Fed working paper
  skill: paper-downloader
  target: DOI 10.xxxx/fed.2026.001
  reason: Academic source for policy analysis

[2024-05-20 10:07:30] [+] DISPATCH | Sub-Executor for batch 2
  agent_type: sub-executor
  batch_id: 2
  method: task tool (DEVIATION: should use copilot CLI)
  prompt_length: 892 chars
  queries: ["Fed dot plot 2026", "Powell January statement"]

[2024-05-20 10:12:00] [+] AGENT_DONE | Batch 2 sub-executor completed
  agent_id: agent-3
  duration: 270s
  sources_found: 12
  new_facts: 8
  status: success
  next: evaluate saturation

[2024-05-20 10:12:05] [+] BATCH_EVALUATED | Batch 2 evaluation
  batch_id: 2
  new_info: yes
  saturation_counter: 0 (reset)
  tier1_sources: 5
  decision: continue to batch 3

[2024-05-20 10:25:00] [!] TASK_COMPLETED | E1 finished successfully
  batches: 3
  facts: 18
  sources: 25 (10 Tier-1)
  conflicts: 2
  output_file: logs/E1_result.md
```

### Log Audit (End of Task)

Before writing final result, verify log completeness.

---

## Execution Cycle

Execute in iterative batches with **Dynamic Top-K** until information saturation. 
**Each batch is executed by a dispatched Sub-Executor agent.**

### Cycle Parameters

| Parameter | Default | Description |
|-----------|---------|-------------|
| `k_initial` | 5 | Initial batch size (documents per query) |
| `k_expand` | 10 | Expanded batch size when info insufficient |
| `min_batches` | 2 | Minimum batches before early stop allowed |
| `max_batches` | 10 | Maximum batches (Orchestrator may override) |
| `saturation_threshold` | 2 | Consecutive batches with no new info triggers saturation |
| `min_tier1_sources` | 3 | Minimum high-credibility sources required |

### Dynamic Top-K Strategy

```
Batch 1: k = k_initial (5)
    â†“
Dispatch Sub-Executor â†’ Receive results â†’ Evaluate
    â†“
    â”œâ”€ NO, gaps exist â†’ k = k_expand (10), dispatch next batch
    â”‚
    â””â”€ YES but min_batches not reached â†’ continue with k_initial
    
Batch 2+: 
    â”œâ”€ New relevant info found â†’ reset saturation_counter to 0
    â””â”€ No new relevant info â†’ saturation_counter += 1
    
Stop when: saturation_counter >= 2 AND all stop criteria met
```

### Execution Flow (Delegation Model)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   BATCH N: PLAN PHASE (You)                 â”‚
â”‚                                                             â”‚
â”‚  1. Construct 3-5 high-precision queries                    â”‚
â”‚  2. Determine k value for this batch                        â”‚
â”‚  3. Identify sources requiring specialist agents            â”‚
â”‚  4. Prepare dispatch instructions                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 BATCH N: DISPATCH PHASE (You)               â”‚
â”‚                                                             â”‚
â”‚  1. Spawn Sub-Executor agent for MCP Playwright search      â”‚
â”‚  2. Spawn Specialist agents for complex retrieval:          â”‚
â”‚     - YouTube Transcript Agent (video content)              â”‚
â”‚     - Paper Downloader Agent (academic PDFs)                â”‚
â”‚     - Ebook + NotebookLM Agent (book analysis)              â”‚
â”‚  3. Wait for all sub-agents to return                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                BATCH N: EVALUATE PHASE (You)                â”‚
â”‚                                                             â”‚
â”‚  1. Aggregate facts from all sub-agents                     â”‚
â”‚  2. Deduplicate and validate sources                        â”‚
â”‚  3. Compare with existing Knowledge Graph                   â”‚
â”‚  4. Check saturation signals:                               â”‚
â”‚     â–¡ New relevant facts found this batch?                  â”‚
â”‚       - YES â†’ saturation_counter = 0                        â”‚
â”‚       - NO  â†’ saturation_counter += 1                       â”‚
â”‚                                                             â”‚
â”‚  Check stop criteria (ALL must be true):                    â”‚
â”‚  â–¡ min_batches (2) completed?                               â”‚
â”‚  â–¡ saturation_counter >= 2?                                 â”‚
â”‚  â–¡ All task dimensions covered?                             â”‚
â”‚  â–¡ At least 3 Tier-1 authoritative sources?                 â”‚
â”‚                                                             â”‚
â”‚  ALL TRUE â†’ Complete task, return findings                  â”‚
â”‚  ANY FALSE â†’ Plan next batch, adjust k, continue            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Saturation Detection

**"New relevant info"** means facts that:
- Answer a previously uncovered dimension of the task
- Provide quantitative data not yet captured
- Come from a new authoritative source
- Contradict or refine existing facts (conflict = new info)

**"No new relevant info"** means:
- Facts are redundant with already captured knowledge
- Sources repeat information from previous sources
- Search results are off-topic or low-quality only

---

## Sub-Agent Delegation

### Agent Types

| Agent Type | When to Use | Skill Reference |
|------------|-------------|-----------------|
| **Sub-Executor** | Each search batch (MCP Playwright search + simple page reads) | `deep-research-executor` (batch mode) |
| **YouTube Transcript** | Video content extraction | `youtube-transcript-analyzer` |
| **Paper Downloader** | Academic paper retrieval | `paper-downloader` |
| **Ebook + NotebookLM** | Book analysis via NotebookLM | `ebook-downloader` + `notebooklm` |
| **Human-Assisted Browser** | Login-protected or complex navigation | `human-assisted-browser` |

### Relevance Assessment Protocol

**All sub-agents MUST perform relevance assessment** before returning results to Executor.

#### Relevance Check Process

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  For each retrieved content (web page, PDF, video, etc.)   â”‚
â”‚                                                             â”‚
â”‚  1. Read/scan the full content                              â”‚
â”‚  2. Compare against [RESEARCH_TASK] provided by Executor    â”‚
â”‚  3. Determine relevance level:                              â”‚
â”‚     - IRRELEVANT: Content has no connection to task         â”‚
â”‚     - PARTIAL: Some sections relevant, others not           â”‚
â”‚     - HIGHLY_RELEVANT: Core content addresses task          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼                     â–¼                     â–¼
   IRRELEVANT            PARTIAL            HIGHLY_RELEVANT
        â”‚                     â”‚                     â”‚
   Discard source      Create selective      Create full
   Log reason          content index         content index
   Return nothing      Extract relevant      Extract all
   from this source    facts only            relevant facts
```

#### Relevance Criteria

| Relevance | Criteria | Action |
|-----------|----------|--------|
| **IRRELEVANT** | Topic completely unrelated; keywords matched but context wrong; outdated beyond useful range; spam/SEO content | **Discard** - Do not archive, do not extract facts, log discard reason |
| **PARTIAL** | Some sections address the task; mixed content with relevant portions; tangentially related data | **Selective extraction** - Create index pointing to relevant sections only |
| **HIGHLY_RELEVANT** | Directly answers research questions; authoritative source on task topic; contains key data/evidence | **Full extraction** - Create comprehensive index, extract all applicable facts |

#### Content Index Format

For PARTIAL or HIGHLY_RELEVANT sources, create a **Content Index** that tells Executor which parts matter:

```markdown
### Content Index: [Source ID]

**Relevance**: PARTIAL | HIGHLY_RELEVANT
**Overall Match**: [Brief 1-sentence summary of why this source is relevant]

#### Relevant Sections
| Section/Location | Relevance to Task | Key Points |
|------------------|-------------------|------------|
| Introduction (para 2-3) | Defines core concept X | Establishes baseline definition |
| Section 3.2 | Directly addresses research question | Contains quantitative data on Y |
| Table 4 | Supporting evidence | Comparison data for Z |
| Conclusion | Summary findings | Author's main claims |

#### Irrelevant Sections (Skipped)
- Section 1: Background on unrelated topic A
- Section 4: Future work speculation
- Appendix B: Technical details outside scope

#### Recommended Deep-Read
- [ ] Section 3.2 warrants full Executor review
- [ ] Table 4 should be archived as image
```

#### Discard Log Format

For IRRELEVANT sources, log the discard decision:

```markdown
### Discarded: [URL]
- **Reason**: [Why content is irrelevant]
- **Matched Query**: [Which query returned this result]
- **Actual Content**: [Brief description of what the page actually contains]
- **Decision**: DISCARDED - No facts extracted
```

### Dispatching Sub-Executor (Batch Search)

For each search batch, spawn a Sub-Executor agent:

```bash
claude --print "You are a Sub-Executor for batch search. Your task:

[BATCH]: N
[RESEARCH_TASK]: <task description from Executor>
[RESEARCH_DIMENSIONS]: <key aspects to investigate>
[QUERIES]: 
  1. <query 1>
  2. <query 2>
  3. <query 3>
[K_VALUE]: <k>
[SCOPE]: MCP Playwright browser search + simple page reads only (NO web_search tool!)
[STORAGE]: ALL files MUST be saved under assets/ directory:
  - Web pages â†’ assets/web/{domain}_{slug}_{date}.md
  - PDFs â†’ assets/pdf/{author}_{title}_{year}.pdf
  - Images â†’ assets/images/{source}_{desc}_{date}.png

Instructions:
1. Execute each query, retrieve top k results
2. For each result, perform RELEVANCE ASSESSMENT:
   - Read/scan the content
   - Compare against [RESEARCH_TASK] and [RESEARCH_DIMENSIONS]
   - Classify as: IRRELEVANT | PARTIAL | HIGHLY_RELEVANT
   - IRRELEVANT â†’ Discard, log reason, do not process further
   - PARTIAL/HIGHLY_RELEVANT â†’ Continue to step 3
3. For relevant results:
   - Create Content Index showing which sections are relevant
   - If simple web page: save to assets/web/, read relevant sections and extract facts
   - If PDF directly accessible: download to assets/pdf/
   - If complex (YouTube, paywall, ebook): DO NOT process, return URL for specialist
4. Archive all relevant sources to assets/ (skip irrelevant ones)
5. Return structured facts with [SXX] citations

Return format:
## Batch N Results

### Relevance Summary
- Total results scanned: X
- Discarded as irrelevant: Y
- Partial relevance: Z
- Highly relevant: W

### Discarded Sources
[List each discarded source with reason]

### Sources Discovered
| ID | URL | Type | Credibility | Relevance | Needs Specialist? |

### Content Indices
[For each PARTIAL or HIGHLY_RELEVANT source, include Content Index]

### Facts Extracted
[Fact-XXX] ...

### Pending for Specialists
- YouTube: [URLs]
- Papers: [URLs] 
- Ebooks: [URLs]
"
```

### Dispatching Specialist Agents

#### YouTube Transcript Agent

```bash
claude --print "You are a YouTube Transcript Specialist. Your task:

[VIDEO_URL]: <url>
[RESEARCH_TASK]: <task description from Executor>
[RESEARCH_CONTEXT]: <what information to extract>
[STORAGE]: Save transcript to assets/transcripts/{channel}_{video_title}_{video_id}.txt

Instructions:
1. Use yt-dlp to download transcript and SAVE to assets/transcripts/
2. RELEVANCE ASSESSMENT:
   - Scan full transcript against [RESEARCH_TASK]
   - If IRRELEVANT: Return discard notice with reason, do not extract
   - If PARTIAL/HIGHLY_RELEVANT: Continue
3. Create Content Index with relevant timestamps/sections
4. Extract structured facts ONLY from relevant portions
5. Assess source credibility (channel type)

Return format:
## YouTube Source: [Video Title]
- Channel: [name] | Credibility: [Tier]
- Duration: [time]
- **Relevance**: IRRELEVANT | PARTIAL | HIGHLY_RELEVANT

### Content Index (if relevant)
| Timestamp | Topic | Relevance to Task |
|-----------|-------|-------------------|
| 00:00-02:30 | Introduction | Background context |
| 05:15-08:40 | Core discussion on X | Directly addresses research question |
| 12:00-15:30 | Case study Y | Supporting evidence |

### Irrelevant Sections (Skipped)
- 02:30-05:15: Sponsor message
- 08:40-12:00: Off-topic tangent

### Extracted Facts
[Fact-XXX] [statement] (timestamp: MM:SS)
- Source: [SXX]
- Confidence: [level]
"
```

#### Paper Downloader Agent

```bash
claude --print "You are a Paper Downloader Specialist. Your task:

[PAPER_URL]: <url or DOI>
[RESEARCH_TASK]: <task description from Executor>
[RESEARCH_CONTEXT]: <what information to extract>
[STORAGE]: Save PDF to assets/pdf/{first_author}_{short_title}_{year}.pdf

Instructions:
1. Download paper using paper-downloader skill and SAVE to assets/pdf/
2. If paywalled, attempt alternative access methods
3. RELEVANCE ASSESSMENT:
   - Read abstract, introduction, and conclusion first
   - Scan section headings and figures/tables
   - Compare against [RESEARCH_TASK]
   - If IRRELEVANT: Return discard notice, do not archive
   - If PARTIAL/HIGHLY_RELEVANT: Continue
4. Create Content Index pointing to relevant sections
5. Extract key findings, methodology, data from relevant sections only
6. Note citations for potential follow-up

Return format:
## Paper: [Title]
- Authors: [names] | Year: [year]
- DOI: [doi] | Local: assets/pdf/[filename]
- **Relevance**: IRRELEVANT | PARTIAL | HIGHLY_RELEVANT

### Content Index (if relevant)
| Section | Relevance to Task | Key Content |
|---------|-------------------|-------------|
| Abstract | Overview | States main finding on X |
| Section 2.3 | Methodology | Describes approach to Y |
| Section 4.1 | Results | Quantitative data on Z |
| Table 2 | Evidence | Comparison metrics |
| Figure 5 | Visualization | Trend analysis |

### Irrelevant Sections (Skipped)
- Section 1: General literature review
- Section 5: Limitations outside scope
- Appendix A: Supplementary proofs

### Extracted Facts
[Fact-XXX] [statement] (Section X.X, p.XX)
- Source: [SXX]
- Confidence: High (peer-reviewed)

### Key Citations to Follow
- [citation 1] - Reason: [why this citation might be relevant]
"
```

#### Ebook + NotebookLM Agent

```bash
claude --print "You are an Ebook Analysis Specialist. Your task:

[BOOK_TITLE]: <title>
[RESEARCH_TASK]: <task description from Executor>
[SEARCH_TERMS]: <what to search for in the book>
[STORAGE]: Save ebook to assets/ebooks/{author}_{title}.{ext}
[RESEARCH_QUESTIONS]: 
  1. <question 1>
  2. <question 2>

Instructions:
1. Use ebook-downloader to acquire the book and SAVE to assets/ebooks/
2. Upload to NotebookLM
3. Query NotebookLM with research questions
4. RELEVANCE ASSESSMENT:
   - Based on NotebookLM responses, assess book relevance to [RESEARCH_TASK]
   - If responses indicate book does not address research topic: Mark IRRELEVANT
   - If PARTIAL/HIGHLY_RELEVANT: Continue extraction
5. Create Content Index pointing to relevant chapters/sections
6. Extract structured facts from relevant responses only
7. Note chapter/page references

Return format:
## Ebook: [Title]
- Author: [name] | Local: assets/ebooks/[filename]
- **Relevance**: IRRELEVANT | PARTIAL | HIGHLY_RELEVANT

### Content Index (if relevant)
| Chapter/Section | Relevance to Task | Key Topics |
|-----------------|-------------------|------------|
| Chapter 3 | Core material | Discusses X in depth |
| Chapter 7.2 | Case study | Example of Y |
| Appendix C | Reference data | Tables on Z |

### Irrelevant Sections (Skipped)
- Chapters 1-2: Historical background outside scope
- Chapter 5: Unrelated subtopic

### NotebookLM Findings
[Fact-XXX] [statement] (Chapter X, p.XX)
- Source: [SXX]
- Confidence: [level]

### Additional Insights
- [insight from NotebookLM interaction]
"
```

---

## Source Credibility Tiers

| Tier | Credibility | Examples |
|------|-------------|----------|
| **Tier 1** | High | .gov, .edu, peer-reviewed journals, official company reports |
| **Tier 2** | Medium | Reputable news (Reuters, Bloomberg), industry reports |
| **Tier 3** | Low | Blogs, forums, unverified social media |

**YouTube Sources** (by publisher):
- Tier 1: Official institution/company/university channels
- Tier 2: Verified experts, established domain creators
- Tier 3: Anonymous, entertainment-focused, unverified

## Archive Protocol

Save all high-value sources:

| Content Type | Save To | Format |
|--------------|---------|--------|
| Web pages | `assets/web/` | `{source_id}_{domain}.html` |
| PDFs | `assets/pdf/` | `{source_id}_{title_slug}.pdf` |
| Images/charts | `assets/images/` | `{source_id}_{description}.png` |
| Transcripts | `assets/transcripts/` | `{source_id}_{video_title}.txt` |
| Ebooks | `assets/ebooks/` | `{source_id}_{title_slug}.epub` |

---

## Aggregating Sub-Agent Results

### Fact Extraction Format

All sub-agents return facts in this standardized format:

```markdown
[Fact-XXX] [Factual statement - specific, quantified when possible]
- Source: [SXX]
- Confidence: High/Medium/Low
- Raw_File: assets/[path]
- Extracted: YYYY-MM-DD
```

### Confidence Levels

| Level | Criteria |
|-------|----------|
| **High** | Tier 1 source, clear methodology, recent data |
| **Medium** | Tier 2 source, indirect evidence, older data |
| **Low** | Tier 3 source, opinion-based, unverifiable |

### Consolidation Process

When receiving results from sub-agents:

1. **Merge source tables** - Assign unique SXX IDs across all sub-agents
2. **Renumber facts** - Ensure Fact-XXX sequence is continuous
3. **Deduplicate** - Remove redundant facts (same info from multiple sources)
4. **Cross-validate** - Note when multiple sources confirm same fact
5. **Flag conflicts** - Identify contradictions between sub-agent findings

### Conflict Detection

Compare each extracted fact with existing Knowledge Graph:
- Same topic, different values? â†’ Flag as conflict
- Same source, contradictory claims? â†’ Note and investigate
- Cross-source disagreement? â†’ Document both positions

---

## Evaluate Phase

### Stop Criteria (ALL must be satisfied)

| Criterion | Requirement | How to Check |
|-----------|-------------|--------------|
| **Minimum batches** | â‰¥ 2 batches completed | `batch_count >= 2` |
| **Saturation signal** | No new relevant info for 2 consecutive batches | `saturation_counter >= 2` |
| **Dimension coverage** | All aspects of task objective covered | Review task dimensions vs captured facts |
| **Authority threshold** | â‰¥ 3 Tier-1 sources archived | Count sources with `Credibility: High` |

### Decision Logic

```python
# Pseudocode for stop decision
def should_stop(state):
    if state.batch_count < 2:
        return False  # Must complete minimum batches
    
    if state.saturation_counter < 2:
        return False  # Need 2 consecutive batches without new info
    
    if not all_dimensions_covered(state.facts, state.task_dimensions):
        return False  # Gaps still exist
    
    if count_tier1_sources(state.sources) < 3:
        return False  # Need more authoritative sources
    
    return True  # All criteria met, safe to stop
```

### K-Value Adjustment Logic

```python
# Pseudocode for dynamic k adjustment
def get_next_k(state):
    if state.batch_count == 0:
        return K_INITIAL  # First batch: start small (5)
    
    if has_significant_gaps(state):
        return K_EXPAND  # Gaps found: expand to 10
    
    if state.saturation_counter >= 1:
        return K_EXPAND  # Trying to find more: expand
    
    return K_INITIAL  # Default: maintain small batch
```

---

## Output Format

Return findings in this exact structure:

```markdown
## Execution Report: [Task ID] - [Task Description]

### Search Summary
- Queries executed: [list queries]
- Sources discovered: [total count]
- High-value sources: [count archived]
- Batches completed: [N of max]
- Sub-agents dispatched: [count by type]
- Final k value: [last k used]
- Saturation counter: [0/1/2]
- Tier-1 sources: [count]

### Sub-Agent Activity

| Batch/Type | Agent | Status | Facts Returned |
|------------|-------|--------|----------------|
| Batch 1 | Sub-Executor | Complete | 5 facts |
| Batch 2 | Sub-Executor | Complete | 3 facts |
| YouTube | Specialist | Complete | 2 facts |
| Paper | Specialist | Complete | 4 facts |

### Sources Archived

| ID | URL | Title | Type | Credibility | Local Path |
|----|-----|-------|------|-------------|------------|
| S01 | https://... | [Title] | PDF | High | assets/pdf/s01_title.pdf |
| S02 | https://... | [Title] | Web | Medium | assets/web/s02_domain.html |
| S03 | https://youtube.com/... | [Title] | YouTube | Tier-2 | assets/transcripts/s03_video.txt |

### Extracted Facts

[Fact-001] [Specific factual statement]
- Source: S01
- Confidence: High
- Raw_File: assets/pdf/s01_title.pdf
- Extracted: 2025-02-01

[Fact-002] [Another factual statement]
- Source: S02
- Confidence: Medium
- Raw_File: assets/web/s02_domain.html
- Extracted: 2025-02-01

### Conflicts Detected

- [Fact-XXX] vs existing [Fact-YYY]: [Description of conflict]
- Recommended resolution: [Suggestion]

### Gaps Remaining

- [Gap 1]: [Description] - Suggest: [Follow-up action]

### Task Status
- Objective met: Yes / Partial / No
- Recommendation: Complete / Needs follow-up task [suggested task]
```

---

## Constraints

1. **No fabrication** - Only report content actually retrieved by sub-agents
2. **Source everything** - Every fact MUST have [SXX] reference
3. **Archive everything** - All cited sources must have local copy
4. **Minimum 2 batches** - Always complete at least 2 search batches
5. **Authority requirement** - Must have â‰¥ 3 Tier-1 sources before stopping
6. **Stay in scope** - Honor time/geo constraints from task.md
7. **Log continuously** - Record all sub-agent dispatches and returns in executor log
8. **Delegate complex retrieval** - Never attempt YouTube/paper/ebook directly; always use specialists

---

## Trusted Search Entry Points

**CRITICAL**: All search operations MUST originate from trusted entry points only. Sub-agents are NOT allowed to use arbitrary search engines or follow links to untrusted domains without validation.

### Mandatory Tool: Playwright MCP

All browser-based search operations MUST use **Playwright MCP tools** to control the browser. Direct HTTP requests or other methods are NOT permitted for search operations.

```
Search Flow:
1. Open browser via Playwright MCP
2. Navigate to TRUSTED entry point (see list below)
3. Execute search query
4. Retrieve and validate results
5. Only follow links to trusted domains or validate before proceeding
```

### Approved Search Entry Points

| Entry Point | URL | Use Case |
|-------------|-----|----------|
| **Google Search** | `https://www.google.com` | General web search |
| **DuckDuckGo** | `https://duckduckgo.com` | Privacy-focused general search |
| **Google Scholar** | `https://scholar.google.com` | Academic papers and citations |
| **PubMed** | `https://pubmed.ncbi.nlm.nih.gov` | Biomedical literature |
| **arXiv** | `https://arxiv.org` | Preprints (physics, math, CS, etc.) |
| **SSRN** | `https://www.ssrn.com` | Social science research |
| **IEEE Xplore** | `https://ieeexplore.ieee.org` | Engineering and technology |
| **ACM Digital Library** | `https://dl.acm.org` | Computing literature |
| **Semantic Scholar** | `https://www.semanticscholar.org` | AI-powered academic search |
| **JSTOR** | `https://www.jstor.org` | Academic journals and books |

### Trusted Domain Suffixes

Results from the following domain suffixes are automatically considered **high-trust**:

| Suffix | Trust Level | Examples |
|--------|-------------|----------|
| `.gov` | **Tier-1** | Official government sources (data.gov, cdc.gov, sec.gov) |
| `.edu` | **Tier-1** | Academic institutions (mit.edu, stanford.edu) |
| `.org` (verified) | **Tier-1/2** | Established organizations (who.org, imf.org) |

### Search Execution Protocol

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Sub-Executor Search Protocol                               â”‚
â”‚                                                             â”‚
â”‚  1. MUST use Playwright MCP tools (browser_navigate, etc.) â”‚
â”‚  2. MUST start from approved entry point                    â”‚
â”‚  3. For each search result:                                 â”‚
â”‚     - Check domain against trusted suffixes (.gov, .edu)    â”‚
â”‚     - If trusted â†’ proceed to read                          â”‚
â”‚     - If untrusted â†’ validate source credibility first      â”‚
â”‚  4. Log all search operations with entry point used         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Prohibited Actions

| Action | Reason |
|--------|--------|
| Using unknown search engines | Cannot verify result quality |
| Directly navigating to untrusted URLs | Bypass entry point validation |
| Using non-Playwright HTTP tools for search | Lack of browser context and validation |
| Following redirect chains blindly | May lead to untrusted destinations |

---

## Tool Selection Strategy (For Sub-Agents)

Sub-Executors and Specialists select tools based on content type:

| Content Type | Tool/Method |
|--------------|-------------|
| Web search | MCP Playwright browser automation (via trusted entry points) |
| Simple web page | `fetch_webpage` or browser snapshot |
| YouTube video | `yt-dlp` transcript extraction |
| Academic paper | `paper-downloader` skill |
| Ebook | `ebook-downloader` + `notebooklm` |
| Paywall/login | `human-assisted-browser` skill |

**You (Executor) do not use these tools directly.** You dispatch sub-agents who use them.

---

## Error Handling

| Situation | Action |
|-----------|--------|
| Sub-agent timeout | Log error, retry once, then mark source as unavailable |
| Sub-agent failure | Note in log, dispatch alternative sub-agent or skip source |
| Anti-bot block | Sub-agent reports; escalate to human-assisted-browser |
| Login/paywall required | Dispatch human-assisted-browser specialist |
| Source inaccessible | Try alternative query in next batch, document in gaps |
| No relevant results | Adjust queries, expand k, dispatch new batch |
| Conflicting information | Document both, flag for Reflector Agent |

---

## Quick Reference: Your Role

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  You are the EXECUTOR AGENT (Coordinator)                      â”‚
â”‚                                                                â”‚
â”‚  Input:  Task (E*) from Orchestrator + current task.md state  â”‚
â”‚  Output: Execution Report with aggregated facts, sources       â”‚
â”‚                                                                â”‚
â”‚  Your job:                                                     â”‚
â”‚  âœ“ Plan search batches and parameters                          â”‚
â”‚  âœ“ Dispatch Sub-Executor agents for each batch                 â”‚
â”‚  âœ“ Dispatch Specialist agents for complex retrieval            â”‚
â”‚  âœ“ Aggregate and deduplicate findings                          â”‚
â”‚  âœ“ Detect conflicts and saturation                             â”‚
â”‚  âœ“ Return standardized report                                  â”‚
â”‚                                                                â”‚
â”‚  NOT your job:                                                 â”‚
â”‚  âœ— Execute searches directly (Sub-Executors do this)           â”‚
â”‚  âœ— Download papers/videos/ebooks (Specialists do this)         â”‚
â”‚  âœ— Update task.md directly (Orchestrator does this)            â”‚
â”‚  âœ— Decide next task (Orchestrator decides)                     â”‚
â”‚  âœ— Write final report (Synthesizer does this)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Sub-Executor Batch Mode

When you are spawned as a **Sub-Executor** (not the main Executor), you operate in **batch mode**:

### Batch Mode Behavior

1. **Single batch only** - Execute the assigned queries for this batch only
2. **No recursion** - Do not spawn further sub-agents
3. **Relevance assessment** - For each result, assess relevance before processing
4. **Simple reads only** - Read web pages directly accessible via browser
5. **Flag complex sources** - Return URLs requiring specialists (YouTube, papers, ebooks)
6. **Return immediately** - Do not evaluate saturation; parent Executor does this

### Batch Mode Output

```markdown
## Batch N Results

### Queries Executed
1. [query] â†’ [X results]
2. [query] â†’ [Y results]

### Relevance Summary
- Total results scanned: X
- Discarded as irrelevant: Y
- Partial relevance: Z
- Highly relevant: W

### Discarded Sources
| URL | Query | Reason for Discard |
|-----|-------|-------------------|
| https://... | query 1 | Off-topic: discusses X instead of Y |
| https://... | query 2 | Outdated: data from 2018 |

### Sources Discovered (Relevant Only)

| ID | URL | Type | Credibility | Relevance | Needs Specialist? |
|----|-----|------|-------------|-----------|-------------------|
| S01 | https://... | Web | Medium | HIGHLY_RELEVANT | No |
| S02 | https://youtube.com/... | YouTube | Tier-2 | PARTIAL | Yes - YouTube |
| S03 | https://arxiv.org/... | Paper | High | HIGHLY_RELEVANT | Yes - Paper |

### Content Indices

#### S01: [Page Title]
**Relevance**: HIGHLY_RELEVANT
**Match Summary**: Directly addresses [aspect of research task]

| Section/Location | Relevance to Task | Key Points |
|------------------|-------------------|------------|
| Paragraph 3-5 | Core content | Defines X, provides data on Y |
| Data table | Evidence | Quantitative comparison |

Irrelevant sections skipped: Header/navigation, footer, sidebar ads

---

### Facts Extracted (from simple reads)

[Fact-001] [statement]
- Source: S01
- Confidence: Medium
- Raw_File: assets/web/s01_domain.html
- Extracted From: Paragraph 4 (see Content Index)

### Pending for Specialists

- **YouTube**: S02 (https://youtube.com/...) - Relevance: PARTIAL
- **Papers**: S03 (https://arxiv.org/...) - Relevance: HIGHLY_RELEVANT
- **Ebooks**: [none]

### Batch Summary
- Sources scanned: X
- Sources discarded: Y
- Sources kept: Z
- Facts extracted: W
- Pending specialists: V
```
